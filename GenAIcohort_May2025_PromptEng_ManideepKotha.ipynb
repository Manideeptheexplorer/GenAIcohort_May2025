{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMv2NkcioPx25u2BphNQu9C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Manideeptheexplorer/GenAIcohort_May2025/blob/main/GenAIcohort_May2025_PromptEng_ManideepKotha.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DAY-01 GENAI PROMPT ENGINEERING ASSIGNMENTS\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0OkeDiw_WLb5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "##Develop Prompt engineering solutions based on 'Real world use cases':\n",
        "\n",
        "**Beginner - Summary Extraction**\n",
        "\n",
        "Write the summary of the text below-\n",
        "\n",
        "Black..... holes—those mysterious cosmic phenomena—are regions in space with gravitational forces so intense that not even light can escape. Astrophysicists Dr. Alan Rothwell and Prof. Priya Sinha### have dived deeply into the “science” $$$$ of these objects, uncovering fascinating insights! According to Rothwell’s research (2021), black holes form when massive stars burn ^^^^through their fuel & collapse under gravity until they reach a singularity—a point of (nearly) infinite density, where physics bReak dOwn.\n",
        "\n",
        "Sinha’s studies focus on the event horizon, or the “point of no return”; any matter or light crossing this boundary is trapped forever. Sinha also examines Hawking Radiation—a theoretical ....concept from Stephen Hawking—suggesting that black holes.... emit small amounts of energy...., slowly losing mass. Such radiation might allow scientists to peek inside these intense entities!!!!. Together, Rothwell and Sinha’s studies hint at bridging quantum mechanics w/ general relativity—fields otherwise tough to reconcile!\n",
        "\n",
        "Remember to clean the text.\n",
        "\n",
        "****************"
      ],
      "metadata": {
        "id": "e64yFlNpJjyC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q groq"
      ],
      "metadata": {
        "id": "J8jZFvUjQx3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "groq_api_key = userdata.get('groq_api')\n",
        "os.environ[\"GROQ_API_KEY\"] = groq_api_key\n"
      ],
      "metadata": {
        "id": "F83AcAqkQzk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_response(prompt):\n",
        "  #define model\n",
        "  model=\"groq/llama3-70b-8192\"\n",
        "  messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "\n",
        "  #call responses\n",
        "  response = completion(\n",
        "      model=model,\n",
        "      messages=messages\n",
        "  )\n",
        "\n",
        "  return response['choices'][0]['message']['content'] or \"\""
      ],
      "metadata": {
        "id": "kwieTDnbQ9aN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"\"\" Black..... holes—those mysterious cosmic phenomena—are regions in space with gravitational forces so intense that not even light can escape. Astrophysicists Dr. Alan Rothwell and Prof. Priya Sinha### have dived deeply into the “science”\n",
        "\n",
        "of these objects, uncovering fascinating insights! According to Rothwell’s research (2021), black holes form when massive stars burn ^^^^through their fuel & collapse under gravity until they reach a singularity—a point of (nearly) infinite density, where physics bReak dOwn.\n",
        "\n",
        "Sinha’s studies focus on the event horizon, or the “point of no return”; any matter or light crossing this boundary is trapped forever. Sinha also examines Hawking Radiation—a theoretical ....concept from Stephen Hawking—suggesting that black holes.... emit small amounts of energy...., slowly losing mass. Such radiation might allow scientists to peek inside these intense entities!!!!. Together, Rothwell and Sinha’s studies hint at bridging quantum mechanics w/ general relativity—fields otherwise tough to reconcile!\"\"\""
      ],
      "metadata": {
        "id": "v4hQOu8bxHfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "    # Remove special symbols and multiple punctuation\n",
        "    text = re.sub(r'[^\\w\\s\\.\\,\\-\\']', ' ', text)  # keep alphanumerics and punctuation\n",
        "    text = re.sub(r'\\s+', ' ', text)  # replace multiple spaces with one\n",
        "    text = re.sub(r'\\.\\.+', '.', text)  # replace ellipses with a single period\n",
        "    text = re.sub(r'(?<=\\w)[A-Z]', lambda x: ' ' + x.group(0), text)  # fix camel-case issues\n",
        "    text = re.sub(r'\\s([?.!,\"](?:\\s|$))', r'\\1', text)  # remove space before punctuation\n",
        "    return text.strip()"
      ],
      "metadata": {
        "id": "a-K2A6aRAs1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text=clean_text(text)\n",
        "prompt=f\"\"\"\n",
        "Write the summary of the text below:\n",
        "{text}\n",
        "\"\"\"\n",
        "response=get_response(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aspSgHxZw4SZ",
        "outputId": "d54bcad7-a1db-4bc1-e67b-7ce75fa58a8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here is a summary of the text:\n",
            "\n",
            "Astrophysicists Dr. Alan Rothwell and Prof. Priya Sinha have researched black holes, regions with intense gravity where not even light can escape. Rothwell's work (2021) shows that black holes form when massive stars collapse into a singularity, where physics breaks down. Sinha's research focuses on the event horizon (point of no return) and Hawking Radiation, which suggests that black holes emit small amounts of energy, allowing scientists to peek inside. Their studies together may help bridge the gap between quantum mechanics and general relativity.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Intermediate - Sentiment Analysis**\n",
        "\n",
        "Given a review, design a prompt to perform sentiment analysis and return the sentiment of the writer as Positive, Negative or Neutral.\n",
        "\n",
        "Example-\n",
        "\"I absolutely loved the new update—it’s so user-friendly and fast!\"\n",
        "Output- Positive"
      ],
      "metadata": {
        "id": "OD_nB5cEhpmE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=\"\"\"I absolutely loved the new update—it’s so user-friendly and fast\n",
        "          output:positive\n",
        "          It is absolutely annoying with the new AI updates.\n",
        "          output:negative\n",
        "          I am excited that new open AI model is releasing this month\n",
        "          output:\n",
        "          \"\"\"\n",
        "response=get_response(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQZ6NuuXyBFx",
        "outputId": "21344e54-3962-419d-f863-8c1f977e10ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=\"\"\"Analyze the sentiment of the following review and classify it as one of: Positive, Negative, or Neutral.\n",
        "\n",
        "Review: \"I absolutely loved the new update—it’s so user-friendly and fast!\"\n",
        "Sentiment:\n",
        "\"\"\"\n",
        "response=get_response(prompt)\n",
        "print(response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTViJ7EDz070",
        "outputId": "2cbc8865-f3e7-4e67-b314-0fe96313694d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment: Positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Advanced - Intelligent Resume Filtering using Keyword Extraction**\n",
        "\n",
        "A hiring manager is seeking assistance in filtering potential candidates from thousands of resumes stored in a MongoDB collection. Your task is to help by extracting meaningful keywords from the manager's natural language query, based on predefined filters.\n",
        "\n",
        "Task:\n",
        "Given a query from the hiring manager and a list of filters:\n",
        "Filters = [\"work_experience\", \"technical_skills\", \"designation\", \"education\",\"location\"]\n",
        "\n",
        "You must:\n",
        "\n",
        "1. Extract relevant keywords from the query, mapped to the appropriate filters.\n",
        "\n",
        "2. Construct and return a valid MongoDB query (in PyMongo format) that can be directly executed on the resumes collection to retrieve matching candidates.\n",
        "\n",
        "Example-\n",
        "\n",
        "Hiring Manager Query Samples\n",
        "\n",
        "- \"Get me candidates with 2 years experience in python and flask and ready to work in Banglore\"\n",
        "- skills : python\n",
        "- candidate name Akash agarwal\n",
        "\n",
        "Output:\n",
        "{\n",
        "\"work_experience\": {\"$eq\":2},\n",
        "\"technicak_skills\":{\"$in\": [\"Python\", \"flask\"]},\n",
        "\"location\":{\"Banglore\"}\n",
        "}\n",
        "\n",
        "Remember to add guardrails."
      ],
      "metadata": {
        "id": "N1ajjRklfTbN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=\"\"\"You are a resume query assistant. Your task is to help a hiring manager filter potential candidates from a MongoDB collection named `resumes`. The manager gives you a natural language query, and your job is to extract relevant keywords and construct a valid PyMongo-style MongoDB query.\n",
        "\n",
        "You must:\n",
        "- Use only the following filters: [\"work_experience\", \"technical_skills\", \"designation\", \"education\", \"location\"]\n",
        "- Map the keywords from the query to the appropriate filter\n",
        "- Normalize common terms (e.g., map \"2 years experience\" to \"work_experience\": {\"$eq\": 2})\n",
        "- Represent \"skills\" as a list under \"technical_skills\" using `$in`\n",
        "- Ignore candidate names or unrelated information\n",
        "- Only use filters that are present in the query (don't add empty ones)\n",
        "- Output only a single valid Python dictionary representing the MongoDB query\n",
        "- If information is ambiguous, make reasonable assumptions and comment them in the output\n",
        "\n",
        "Example input:\n",
        "\n",
        "Hiring Manager Query: \"Get me candidates with 2 years experience in python and flask and ready to work in Banglore\"\n",
        "\n",
        "Output:\n",
        "{\n",
        "  \"work_experience\": { \"$eq\": 2 },\n",
        "  \"technical_skills\": { \"$in\": [\"Python\", \"Flask\"] },\n",
        "  \"location\": \"Bangalore\"\n",
        "}\n",
        "\n",
        "Now process this query:\n",
        "\"Get me candidates with 5 years experience in python and AI and ready to work in Hyd\"\n",
        "\"\"\"\n",
        "response=get_response(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "Zo9PE4TcDznY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c87fdf0-cfc9-4caa-bcfa-eebe67534eaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here is the output:\n",
            "\n",
            "{\n",
            "  \"work_experience\": { \"$eq\": 5 },\n",
            "  \"technical_skills\": { \"$in\": [\"Python\", \"AI\"] },\n",
            "  \"location\": \"Hyd\"  # Assuming Hyd is short for Hyderabad\n",
            "}\n",
            "\n",
            "Let me know if this is correct!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DAY-02 GENAI CHAIN AND MEMORY ASSIGNMENT"
      ],
      "metadata": {
        "id": "9EwY3R8g9bdh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q groq langchain-groq langchain langchain-community litellm transformers langgraph"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75CDKCCI-v0_",
        "outputId": "36b6fb71-aeb6-4510-cdf6-8dc6dade0eec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.6/129.6 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.0/8.0 MB\u001b[0m \u001b[31m72.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.4/152.4 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.2/44.2 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.0/50.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.5/216.5 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "groq_api_key = userdata.get('groq_api')\n",
        "os.environ[\"GROQ_API_KEY\"] = groq_api_key"
      ],
      "metadata": {
        "id": "243QU9-u-2wO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "\n",
        "llm = ChatGroq(\n",
        "    model=\"llama-3.1-8b-instant\",\n",
        "    temperature=0, # range of temperature variable is from 0(least artistic) to 1(most artistic/enhanced)\n",
        "    max_tokens=None,\n",
        "    timeout=None,\n",
        "    max_retries=2,\n",
        ")"
      ],
      "metadata": {
        "id": "_VLY7gRl-5Yy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conversation Summary\n",
        "\n",
        "Summarizes the conversation and stores the current summary in memory.\n",
        "\n",
        "Explore it yourself!"
      ],
      "metadata": {
        "id": "AjH_PKwyegus"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationSummaryMemory\n",
        "from langchain.chains import ConversationChain"
      ],
      "metadata": {
        "id": "jQmny25afHX4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary_memory = ConversationSummaryMemory(llm=llm)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WdVZdGUjfHvk",
        "outputId": "ea82800e-98dc-428f-a4ed-bbe44232f52c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-def8f97cd92d>:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  summary_memory = ConversationSummaryMemory(llm=llm)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=summary_memory,\n",
        "    verbose=True  # shows intermediate steps\n",
        ")\n"
      ],
      "metadata": {
        "id": "VrS2lKWNgIyD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76c94295-8437-48ac-a44a-1cd90b796581"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-51ef77fb098b>:1: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :meth:`~RunnableWithMessageHistory: https://python.langchain.com/v0.2/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html` instead.\n",
            "  conversation = ConversationChain(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"Hi, I’m Manideep.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "WINdp8DPgSMq",
        "outputId": "fb97090a-243b-45bf-f577-a9519d3d2019"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Hi, I’m Manideep.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Nice to meet you, Manideep. I'm an artificial intelligence designed to assist and communicate with humans in a friendly and informative manner. I've been trained on a vast corpus of text data, which includes but is not limited to, books, articles, research papers, and even conversations like this one. I'm a large language model, which means I can understand and respond to natural language inputs, and I'm constantly learning and improving my responses based on the interactions I have with users like you. What brings you here today, Manideep?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"I’m working on an AI project involving LangChain.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "FGhJ7xbNgZ-0",
        "outputId": "1435f151-737d-4c0d-8367-e0963d2ac99b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Current summary:\n",
            "Manideep introduces himself to the AI, and the AI responds by introducing itself as a friendly and informative artificial intelligence designed to assist and communicate with humans.\n",
            "\n",
            "New lines of conversation:\n",
            "Human: Hi, I’m Manideep.\n",
            "AI: Nice to meet you, Manideep. I'm an artificial intelligence designed to assist and communicate with humans in a friendly and informative manner. I've been trained on a vast corpus of text data, which includes but is not limited to, books, articles, research papers, and even conversations like this one. I'm a large language model, which means I can understand and respond to natural language inputs, and I'm constantly learning and improving my responses based on the interactions I have with users like you. What brings you here today, Manideep?\n",
            "\n",
            "New summary:\n",
            "Manideep introduces himself to the AI, and the AI responds by introducing itself as a friendly and informative artificial intelligence designed to assist and communicate with humans. The AI explains its capabilities, including its training on a vast corpus of text data and its ability to understand and respond to natural language inputs.\n",
            "Human: I’m working on an AI project involving LangChain.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"LangChain is an open-source framework for building and composing large language models like myself. It allows developers to create complex workflows and applications that leverage the capabilities of multiple language models. LangChain is designed to be highly modular and flexible, making it easy to integrate with other tools and services. It also provides a range of features and tools for tasks such as data loading, model training, and inference.\\n\\nI've been trained on a significant amount of text data related to LangChain, including its documentation, tutorials, and community discussions. However, I'd love to learn more about your specific project and how you're using LangChain. What's the focus of your project, and what challenges are you trying to address?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"Can you summarize what I told you?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "rCroSwFSgbVp",
        "outputId": "83492b4c-9b72-49cf-da9a-0a309bbca19f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Current summary:\n",
            "Manideep introduces himself to the AI, and the AI responds by introducing itself as a friendly and informative artificial intelligence designed to assist and communicate with humans. The AI explains its capabilities, including its training on a vast corpus of text data and its ability to understand and respond to natural language inputs. The AI is familiar with LangChain, an open-source framework for building and composing large language models.\n",
            "\n",
            "New lines of conversation:\n",
            "Human: I’m working on an AI project involving LangChain.\n",
            "AI: LangChain is an open-source framework for building and composing large language models like myself. It allows developers to create complex workflows and applications that leverage the capabilities of multiple language models. LangChain is designed to be highly modular and flexible, making it easy to integrate with other tools and services. It also provides a range of features and tools for tasks such as data loading, model training, and inference.\n",
            "\n",
            "I've been trained on a significant amount of text data related to LangChain, including its documentation, tutorials, and community discussions. However, I'd love to learn more about your specific project and how you're using LangChain. What's the focus of your project, and what challenges are you trying to address?\n",
            "\n",
            "New summary:\n",
            "Manideep introduces himself to the AI, and the AI responds by introducing itself as a friendly and informative artificial intelligence designed to assist and communicate with humans. The AI explains its capabilities, including its training on a vast corpus of text data and its ability to understand and respond to natural language inputs. The AI is familiar with LangChain, an open-source framework for building and composing large language models. The AI provides an overview of LangChain's features and capabilities, and expresses interest in learning more about Manideep's specific project and how he is using LangChain.\n",
            "Human: Can you summarize what I told you?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"You introduced yourself as Manideep, and I responded by introducing myself as a friendly and informative artificial intelligence. I explained my capabilities, including my training on a vast corpus of text data and my ability to understand and respond to natural language inputs. I also mentioned that I'm familiar with LangChain, an open-source framework for building and composing large language models, and provided an overview of its features and capabilities. You then shared that you're working on an AI project involving LangChain, and I expressed interest in learning more about your specific project and how you're using LangChain.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(summary_memory.buffer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6I6ho8z6glA7",
        "outputId": "4608e227-7c6f-4e27-a724-46fd611d34ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current summary:\n",
            "Manideep introduces himself to the AI, and the AI responds by introducing itself as a friendly and informative artificial intelligence designed to assist and communicate with humans. The AI explains its capabilities, including its training on a vast corpus of text data and its ability to understand and respond to natural language inputs. The AI is familiar with LangChain, an open-source framework for building and composing large language models. The AI provides an overview of LangChain's features and capabilities, and expresses interest in learning more about Manideep's specific project and how he is using LangChain.\n",
            "\n",
            "New lines of conversation:\n",
            "Human: Can you summarize what I told you?\n",
            "AI: You introduced yourself as Manideep, and I responded by introducing myself as a friendly and informative artificial intelligence. I explained my capabilities, including my training on a vast corpus of text data and my ability to understand and respond to natural language inputs. I also mentioned that I'm familiar with LangChain, an open-source framework for building and composing large language models, and provided an overview of its features and capabilities. You then shared that you're working on an AI project involving LangChain, and I expressed interest in learning more about your specific project and how you're using LangChain.\n",
            "\n",
            "New summary:\n",
            "Manideep introduces himself to the AI, and the AI responds by introducing itself as a friendly and informative artificial intelligence designed to assist and communicate with humans. The AI explains its capabilities, including its training on a vast corpus of text data and its ability to understand and respond to natural language inputs. The AI is familiar with LangChain, an open-source framework for building and composing large language models. The AI provides an overview of LangChain's features and capabilities, and expresses interest in learning more about Manideep's specific project and how he is using LangChain. The AI then summarizes their previous conversation, confirming that Manideep introduced himself and shared his project details, and that the AI responded with its capabilities and interest in learning more about Manideep's project.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DAY-03 GENAI TOOLS BUILDING ASSIGNMENT"
      ],
      "metadata": {
        "id": "UI-cyEYg_ryA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assignment: Building a Smart Assistant for Virtual Healthcare Support"
      ],
      "metadata": {
        "id": "MWZMstScmuLp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You are hired by a startup called CareConnect, which is building an AI-powered assistant to support virtual healthcare providers. This assistant will help nurses, doctors, and patients by analyzing chat transcripts of telehealth sessions.\n",
        "\n",
        "Your job is to design LangChain tools and build an intelligent agent that can dynamically invoke the right tool based on what the user is asking.\n",
        "\n",
        " **Problem Statement**:\n",
        "Given a doctor-patient conversation, the AI assistant should help by:\n",
        "\n",
        "\n",
        "*   Detecting the patient's emotional state. Give a score on a scale of 1-5.\n",
        "*   Summarizing the chief complaint and urgency of the patient.\n"
      ],
      "metadata": {
        "id": "tn43IqEmm3uh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q groq langchain-groq langchain langchain-community langgraph litellm"
      ],
      "metadata": {
        "id": "j5IHlkCojtRW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57b63381-682b-4fd4-c72a-cd6b930b8097"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.6/129.6 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.4/152.4 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.0/8.0 MB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.2/44.2 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.0/50.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.5/216.5 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "groq_api_key = userdata.get('groq_api')\n",
        "os.environ[\"GROQ_API_KEY\"] = groq_api_key"
      ],
      "metadata": {
        "id": "uAfMDsfOBVdl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "from langchain_core.tools import tool\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain_core.messages import HumanMessage, ToolMessage, AIMessage\n",
        "from langgraph.prebuilt import create_react_agent\n",
        "from typing import List\n",
        "from litellm import completion\n",
        "from langchain.agents import initialize_agent, Tool, AgentType"
      ],
      "metadata": {
        "id": "bJabnZNpBglT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain_groq import ChatGroq\n",
        "\n",
        "llm = ChatGroq(\n",
        "    model=\"llama-3.1-8b-instant\", #\"llama-3.1-8b-instant,llama-3.3-70b-versatile\",\n",
        "    temperature=0, # range of temperature variable is from 0(least artistic) to 1(most artistic/enhanced)\n",
        "    max_tokens=None,\n",
        "    timeout=None,\n",
        "    max_retries=2,\n",
        ")"
      ],
      "metadata": {
        "id": "mITTYoyMBqav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, Field\n",
        "\n",
        "class EmotionalStateOutput(BaseModel):\n",
        "    \"\"\"\n",
        "    Output schema for patient's emotional analysis.\n",
        "    \"\"\"\n",
        "    emotion: str = Field(description=\"The emotional state of the patient (e.g., anxious, calm, frustrated).\")\n",
        "    score: int = Field(description=\"Emotion severity score from 1 (very distressed) to 5 (very calm).\")\n"
      ],
      "metadata": {
        "id": "3X92oeHJD4ax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import PydanticOutputParser\n",
        "\n",
        "emotion_parser = PydanticOutputParser(pydantic_object=EmotionalStateOutput)\n"
      ],
      "metadata": {
        "id": "y7ToRx2aEGYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.tools import tool\n",
        "\n",
        "@tool\n",
        "def analyze_patient_emotion(conversation: str) -> EmotionalStateOutput:\n",
        "    \"\"\"\n",
        "    Analyze a doctor–patient conversation to determine the patient's emotional state and score (1–5).\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"\n",
        "    You are an expert emotional state analyst for virtual healthcare.\n",
        "\n",
        "    Given the following doctor–patient conversation:\n",
        "    \\\"\\\"\\\"{conversation}\\\"\\\"\\\"\n",
        "\n",
        "    Your task:\n",
        "    1. Detect the patient's primary emotional state (e.g., anxious, calm, sad, confused).\n",
        "    2. Provide an emotion score from 1 to 5, where 1 is highly distressed and 5 is very calm.\n",
        "    3. Do not include any explanation — just output in this exact format:\n",
        "\n",
        "    {emotion_parser.get_format_instructions()}\n",
        "    \"\"\"\n",
        "\n",
        "    response = llm.invoke(prompt)\n",
        "    return emotion_parser.parse(response.content.strip())\n"
      ],
      "metadata": {
        "id": "I7DdcYzBEH8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ComplaintUrgencyOutput(BaseModel):\n",
        "    \"\"\"\n",
        "    Output schema for summarizing chief complaint and urgency.\n",
        "    \"\"\"\n",
        "    complaint_summary: str = Field(description=\"Short summary of the patient's main issue.\")\n",
        "    urgency: str = Field(description=\"Urgency level: emergency, urgent, routine, or non-urgent.\")\n"
      ],
      "metadata": {
        "id": "9pTTTTgOEY-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "complaint_parser = PydanticOutputParser(pydantic_object=ComplaintUrgencyOutput)\n"
      ],
      "metadata": {
        "id": "TvtsfZfHEbeh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tool\n",
        "def summarize_complaint_and_urgency(conversation: str) -> ComplaintUrgencyOutput:\n",
        "    \"\"\"\n",
        "    Summarizes the main health issue (chief complaint) and evaluates the urgency level.\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"\n",
        "    You are a virtual healthcare assistant specializing in medical triage.\n",
        "\n",
        "    Given the following doctor–patient conversation:\n",
        "    \\\"\\\"\\\"{conversation}\\\"\\\"\\\"\n",
        "\n",
        "    Your task:\n",
        "    1. Summarize the patient's main complaint (e.g., chest pain, sore throat, dizziness).\n",
        "    2. Assess the urgency of the situation. Choose one: emergency, urgent, routine, or non-urgent.\n",
        "    3. Respond **only** in the following JSON format:\n",
        "\n",
        "    {complaint_parser.get_format_instructions()}\n",
        "    \"\"\"\n",
        "\n",
        "    response = llm.invoke(prompt)\n",
        "    return complaint_parser.parse(response.content.strip())\n"
      ],
      "metadata": {
        "id": "4vFtobcrEgH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = summarize_complaint_and_urgency(conversation)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6FgGFoF_HBCp",
        "outputId": "70a6dc9d-7b61-4302-e1c0-41620ff66d32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-23-70506c7a5e6a>:1: LangChainDeprecationWarning: The method `BaseTool.__call__` was deprecated in langchain-core 0.1.47 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  result = summarize_complaint_and_urgency(conversation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "complaint_summary='Chest pain, shortness of breath, and dizziness' urgency='emergency'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tools = [\n",
        "    Tool(\n",
        "        name=\"PatientEmotionAnalyzer\",\n",
        "        func=analyze_patient_emotion,\n",
        "        description=\"Use this when you need to detect the emotional state of the patient and provide a score.\"\n",
        "    ),\n",
        "    Tool(\n",
        "        name=\"ComplaintUrgencySummarizer\",\n",
        "        func=summarize_complaint_and_urgency,\n",
        "        description=\"Use this to extract the patient's chief complaint and evaluate how urgent the issue is.\"\n",
        "    )\n",
        "]\n"
      ],
      "metadata": {
        "id": "UOk1MmYwE7Kp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_with_new_tools = llm.bind_tools(tools)"
      ],
      "metadata": {
        "id": "tkVC3-2RFgeK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent_executor = create_react_agent(llm_with_new_tools,tools)"
      ],
      "metadata": {
        "id": "fcggA0boFuqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation = \"\"\"\n",
        "Doctor: Hello, how can I help you today?\n",
        "Patient: I've been having chest pain for the last 2 days, and it's getting worse.\n",
        "Doctor: Is the pain sharp or dull?\n",
        "Patient: It’s a sharp pain that comes and goes. It’s worse when I take deep breaths.\n",
        "Doctor: Have you experienced shortness of breath or dizziness?\n",
        "Patient: Yes, both. I'm really scared, honestly.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "QJKIj8Y6FEMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"\n",
        "Analyze the following doctor–patient conversation:\n",
        "\\\"\\\"\\\"{conversation}\\\"\\\"\\\"\n",
        "\n",
        "I want you to:\n",
        "1. Detect the patient's emotional state and give a score (1–5).\n",
        "2. Summarize the chief complaint and assess the urgency level (emergency, urgent, routine, or non-urgent).\n",
        "\n",
        "Provide both outputs in one combined result.\n",
        "\"\"\"\n",
        "\n",
        "response = agent_executor.invoke({\n",
        "    \"input\": prompt,\n",
        "    \"messages\": [HumanMessage(content=prompt)]\n",
        "})\n",
        "\n",
        "print(response[\"messages\"][-1].content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DkSGLvUDHTGG",
        "outputId": "0dd30e7b-da39-447a-dd88-ceb8a3edb64c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The combined result is:\n",
            "\n",
            "The patient's emotional state is anxious with a score of 1.\n",
            "The chief complaint is chest pain that worsens with deep breaths, and the urgency level is urgent.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#FINAL WEEK-01 ASSIGNMENT\n"
      ],
      "metadata": {
        "id": "-J3EMT6kMuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "2 Customer Review Extraction- Team #2\n",
        "\n",
        "Extract the customer(name & ID), product and company details\n",
        "Extract the sentiment of the customer\n",
        "What was good or bad about the product- give specific details"
      ],
      "metadata": {
        "id": "LVZyRDXvNoeS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import json\n",
        "from typing import List\n",
        "from pydantic import BaseModel, Field, ValidationError\n",
        "\n",
        "\n",
        "\n",
        "# === Step 1: Define Pydantic Models ===\n",
        "class CustomerInfo(BaseModel):\n",
        "    name: str\n",
        "    id: str\n",
        "\n",
        "class ProductInfo(BaseModel):\n",
        "    name: str\n",
        "    id: str\n",
        "\n",
        "class ReviewAnalysis(BaseModel):\n",
        "    customer: CustomerInfo\n",
        "    product: ProductInfo\n",
        "    company: str\n",
        "    sentiment: str  # e.g., positive, neutral, negative\n",
        "    good_points: List[str]\n",
        "    bad_points: List[str]\n",
        "\n",
        "\n",
        "\n",
        "# === Step 3: Define the function with JSON recovery ===\n",
        "def extract_review_insights(review: str) -> ReviewAnalysis:\n",
        "    prompt = f\"\"\"\n",
        "You are a review analysis expert.\n",
        "\n",
        "Extract the following details from the given customer review, and return ONLY a valid JSON object with no explanation or comments.\n",
        "\n",
        "Details to extract:\n",
        "- Customer name and ID\n",
        "- Product name and ID\n",
        "- Company/store name\n",
        "- Overall sentiment (positive, neutral, or negative)\n",
        "- Good points as a list\n",
        "- Bad points as a list (if none, return an empty list)\n",
        "\n",
        "Customer review:\n",
        "\\\"\\\"\\\"{review}\\\"\\\"\\\"\n",
        "\n",
        "Only return a valid JSON like this (no commentary!):\n",
        "{{\n",
        "  \"customer\": {{\n",
        "    \"name\": \"...\",\n",
        "    \"id\": \"...\"\n",
        "  }},\n",
        "  \"product\": {{\n",
        "    \"name\": \"...\",\n",
        "    \"id\": \"...\"\n",
        "  }},\n",
        "  \"company\": \"...\",\n",
        "  \"sentiment\": \"...\",\n",
        "  \"good_points\": [\"...\"],\n",
        "  \"bad_points\": [\"...\"]\n",
        "}}\n",
        "\"\"\"\n",
        "    # === Send prompt to LLM ===\n",
        "    response = llm.invoke([HumanMessage(content=prompt)])\n",
        "    raw_output = response.content.strip()\n",
        "\n",
        "    # === Attempt to extract JSON safely ===\n",
        "    try:\n",
        "        # Optional: extract only the first JSON block from the response\n",
        "        json_text = re.search(r\"\\{.*\\}\", raw_output, re.DOTALL).group()\n",
        "        parsed = json.loads(json_text)\n",
        "        return ReviewAnalysis(**parsed)\n",
        "    except (json.JSONDecodeError, ValidationError, AttributeError) as e:\n",
        "        print(\"⚠️ Failed to parse valid JSON. LLM response:\\n\", raw_output)\n",
        "        raise e\n"
      ],
      "metadata": {
        "id": "VPs9Wa-mL6Ar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review_text = \"\"\"\n",
        "@Customer: Varun D. Kapoor (ID#: CRM5543219) | Purchase Date: 22/10/2023 | Store: Croma Electronics, DLF Mall Branch, Sector 18, Noida, UP - 201301 | Product: Pebble BassX Prime™ Wireless Speaker (#PBL-SPK22) | Order #: CRM/DLF/2023/10/7788 ✨ What a FANTASTIC buy @ just ₹2,499/-! ... (full review)\n",
        "\"\"\"\n",
        "\n",
        "result = extract_review_insights(review_text)\n",
        "print(result.model_dump_json(indent=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5y-5YeEvMBtZ",
        "outputId": "91da1cdb-f63b-48bb-ec30-11c5bcaaa1e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"customer\": {\n",
            "    \"name\": \"Varun D. Kapoor\",\n",
            "    \"id\": \"CRM5543219\"\n",
            "  },\n",
            "  \"product\": {\n",
            "    \"name\": \"Pebble BassX Prime Wireless Speaker\",\n",
            "    \"id\": \"PBL-SPK22\"\n",
            "  },\n",
            "  \"company\": \"Croma Electronics\",\n",
            "  \"sentiment\": \"positive\",\n",
            "  \"good_points\": [\n",
            "    \"What a FANTASTIC buy @ just ₹2,499/-!\"\n",
            "  ],\n",
            "  \"bad_points\": []\n",
            "}\n"
          ]
        }
      ]
    }
  ]
}